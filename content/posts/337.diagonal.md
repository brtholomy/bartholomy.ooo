---
title: "Diagonal Causality"
summary: "What is Self-Organized Criticality?"
slug: diagonal-causality
draft: true
date: 2024-11-25T14:57:59-08:00
thumbnail: "thumbnails/diagonal_thumbnail.png"
images: ["covers/diagonal.png"]
tags: [criticality, neuromorphism, math]
theme: greenteen
---

#### .0

There's plenty of evidence that the brain operates at or near criticality. That most of its function which we currently find mysterious, can be described as aftereffects of a nonlinear dynamic system near its critical moments: it's the best theory for describing longrange correlation, which is coordination between physiologically distal and sparsely connected neural tissue producing simultaneous and coherent response, and the mystery of the shallow-but-wide neuromorphic computational style.

That's more or less accepted, or will soon be. What remains to be shown is how or why longrange correlation could come about in general: for example, its known that fractals are generated by those processes which exhibit *nonintegral dimension*, such as the space-filling line, or the coast of infinite jaggedness. It's important to remind ourselves that fractality means it's possible to traverse from a very large scale to a very small scale without a change of navigational function - meaning that one continuous process or *line of force* is capable of traversing not only great distance but *scale*. This is rare to nonexistent in classical physics.

Mathematicians are content to describe the elegant fractality of the process, but don't ask themselves under what condition a system would ever exhibit something like a "space-filling line" - and even those working within chaos theory or synergetics, are content to say that longrange correlation occurs at criticality but never seem to ask *why*. The answer didn't occur to me until I began thinking of all these systems as *dissipative*: a pot of boiling water is bubbling because the superfluous molecular energy seeks a dissipative path. Bubbling is the liquid phase pushed to the limit of its dissipative load.

#### .1

The original model of self-organized criticality is the *sandpile*: simply drop one grain at a time onto a flat surface, and eventually the pile will exhibit fractal structure along with the signature *avalanche* behavior of critical dynamics. As it turns out, it works better with rice - but what's happening? There are three primary forces at work: the friction between any two grains which holds them in place, the gravity pulling them downward, and the addition of new grains as input energy. The result of tension between these forces is expressed in the average maximum slope of the pile. Once this slope is achieved, the pile tends to remain at this slope and expand outward through small collapses. Yet sometimes this slope is exceeded as the pile becomes a little tall, and at this point we expect a larger collapse is due. What's governing this behavior? What we can say for certain, is that there's an *average probability* that any one grain will roll at a given slope: but when one grain rolls, it's also likely to push other grains past their friction limit and start an avalanche. This becomes surprising when we realize that this simple dynamic tends to create both self-similar structure and a distribution curve of avalanche sizes which mirrors many other systems: including the cortical "avalanches" of excitation patterns in the human brain, the foraging patterns of ant colonies, and the responsive cascades of internet traffic.

#### .2

* Low temperature systems exhibit high degrees of order - but not of the same genera as at criticality. A low energy state is orderly in the sense that it's predictable, redundant, crystalline. It has what they call "correlation length", in that given a point, one can swing a radius in any direction and predict the structure: this is the nature of a crystal and tessellation.

* High temperature systems have low degrees of this kind of order, in that one cannot predict the neighboring configuration even given a large sampling of a local area: it has little or no correlation length.

* Critical order is very different, as though it were a hybrid.

The insight comes from the fact that chaos should not be defined as "random probability": this is only a *measurement artifact* and the perspective of a technician dissatisfied with his data, rather than a theory of dynamics. Chaos and disorder in general is a result of *rapid switching* between competing dissipative modes: the appearance of randomness is a result of high degrees of competition such that each path is fragmented, interrupted, partial. A critical phase transition is the balance of multiple modes holding each other in tension, a *crisis* which is in some ways the fuller realization of chaos. This perfect tension produces its own kind of homogeneity: *critical order*, which has scale invariance as a special consequence of its realization.

It could be conceived as a difference between usable and useless chaos: useless chaos occurs when low temperature order is sufficiently disturbed to produce low predictability but not yet scale invariance; useful chaos is a balance between competing dissipative modes at a poised equilibrium, such that the crisis is fully realized at all scales. This realization is itself an expensive mode of dissipation, expensive enough to contain high energetic throughput.

#### .3

I provide now a carefully sorted vocabulary list. These are very loose groupings, designed only to help orient the reader to this vast and disparate field. Each term has different origins, with diverse qualitative implications. Some are supersets of the others. Some are technically valid only in a limited scope, some are almost universal. Each one is worth looking into if its meaning is not clear.

<!-- FORMAT: list short -->

Transition:

* Criticality
* Phase transition
* Susceptibility
* Poise

Recurrence:

* Self-similarity
* Scale invariance
* Inverse power law distribution
* 1/f and pink noise

Dimension:

* Correlation length divergence
* Fractal dimension
* Nonintegral dimension
* Hausdorff dimension
* Critical exponent

Momenta:

* Metastability
* Attractor
* Limit cycle
* Orbital moment

Event:

* Avalanche
* Cascade

Order:

* Phase
* Order parameter
* Coherence
* Redundancy
* Crystallinity
* Oscillation

Disorder:

* Chaos
* Noise
* Stochastic distribution

Feedback:

* Nonlinearity
* Harmonic distortion
* Hysteresis

Amplification:

* Stochastic resonance
* Dither

#### .4

What's happening in a phase change? The quantum of throughput energy is straining the dominant dissipative mode, such that it begins to explore transitions to the next mode. At the point that it falls back from the upper mode into the lower, a kind of sideways path is discovered which accepts a larger quantum of energy than we might expect. This path is constituted by *scale interaction*: the interaction between scales is the dissipation channel immediately before chaos.

#### .5

But what is a "phase"? It's a *mode of dissipation*: even if we imagine the phase of any given substance as a highly stable and energetically conservative system, we should think of every configuration as a rich deposit of energy - every form of order as an expression of the potential for change.

This is primarily what seems to hold people back, in the imagination of how and why criticality might be so ubiquitous: a "phase transition" is not some accidental arbitrary point in the graph, it's the maximum dissipation channel available to an overcrowded and constantly perturbed nonlinear system - to be caught between a disintegrating solidity and a freefall one cannot keep up.

#### .6

What the literature hints at, but does not quite say, is that a fractal arises as a *tessellation of competing dissipative paths*. Like a packing problem in configuration space, the system finds dissipative efficiency via correlation length divergence.

It is as though chaotic fragmentary order were able to accidentally discover this path along the way, as though it's a side-channel of nonlinear dynamics: a tessellated nonintegral path, possible only at the junction between phases.

But why not just transition gradually into the next phase? Why a basin? And thus the metaphor of boiling water in an ordinary pot fails, because *constraint* is required: it must be sufficiently difficult to transition to the next order, such that fractal dimension becomes an easier dissipation path than the gaseous state. This pot must have a lid - and in neural terms, there must be some powerful constraint at work which forces neural systems into the critical path...

#### .7

A few caveats:

<!-- FORMAT: list short -->

* The essence of a fractal is *not* self-similarity, but fractal dimension: Mandelbrot.

* Chaotic systems often follow fractal trajectories known as strange attractors, which show self-similarity, but these are not necessarily caused by criticality: Per Bak.

#### .8

The strange attractor has *nonintegral dimension*. The usual explanation for this remarkable fact relies on analyzing its path in statespace. Given a dissipative system with 3 variables, the attractor must be:

1. Less than 3D to allow for *asymptotic approach*: if the attractor had any volume, it would prevent some trajectories from approaching it asymptotically. It must attract.
2. Greater than 2D to allow for *deterministic neighbor divergence*: if the attractor were a 2D surface, neighboring trajectories could not diverge without intersection - such intersection would mean a "path choice" and the system would no longer be deterministic. It must be sensitive to small changes.

This is why they talk of "stretching and folding": nearby points diverge, distant points converge, and in this dual action the attractor's shape is drawn.

But this argument is backwards, as most mathematical proofs seem to be. It *argues from definitions* to the plotted data, without attempting any real causal theory. Again I lean toward the *dissipative search*: why would a phase transition produce a fractal in statespace? Under what conditions would this be the path of least resistance?

* Why asymptotic approach? Because there is no other optimum dissipation mode. Initial paths will converge here as long the energetic throughput remains steady. An attractor is an order's last compromise with chaos.

* Why neighbor divergence? The usual argument from chaos theory says, "because the system displays sensitive dependence on initial conditions": but that's not an explanation of the dynamics. Why should a system be so sensitive to small differences? Because it is *poised*, because it amplifies friction into signal, because the tension between constraint and input energy propagates perturbation upward and downward.

It's important we understand the difference between fractals in statespace, like strange attractors, and a fractal in material space, like a coastline. Yet another thing I have not seen stated in the literature, is this: most likely the fractals Mandelbrot found in nature are the *traces of statespace fractals*. On the other hand, the argument I just gave for the "sensitive dependence on initial conditions" so prominent in chaos theory - the butterfly in Brazil which causes a tornado in Kansas - shows how the realized material structure must in turn shape the possible evolution of the system. In fact, this may be the secret of complex dynamics: that the future evolution of the system and the traces it leaves behind itself shape one another. That is what we mean by "nonlinear".

<!-- TODO: image of Lorenz attractor -->

#### .9

*Correlation length divergence* can be conceived as a crystallinity skewed to reside between scales, as though between dimensions. An asymptote creates its own nonintegral dimension.

#### .10

How does scale invariance arise? As though by withdrawing from each other, each scale expresses every other. In the expansion itself, the pattern is expressed: by running away into higher and lower scales at every scale, it seeks a space for dissipation. The dissipative expenditure is maintenance of this *diagonal of interaction*: the sum of simultaneous horizontal and vertical scale interactions while the fractal is continuously regenerated. Viewed this way, that nonintegral dimension characteristic of a fractal is *the angle of diagonal scale interaction*: the steeper the angle, the closer the fractal is to the integral n+1 dimension.

In this mode, lines of force or *histories of causality* are able to cross vast distances almost simultaneously, because the entirety of the system is held in a kind of semi-crystalline grip, where cause and effect merge. Changes at small scales ripple upward to large, and vice-versa: this is where such high sensitivity to small perturbations occurs, that it begins to resemble "topdown causality", as though there were a center of agency at the helm, a "kubernetes". But this agency is nomadic and ephemeral, being instantiated at any point where avalanche is initiated: and we finally have a viable theory of agency which accounts both for the mystery of "will" and the nonexistence of self.

#### .11

It's the *fallback* from poised criticality that is so useful: a peak from which hidden valleys can be discovered.

#### .12

They call it "self-organized criticality", but it's not the self-organization which matters: it's the *exquisite responsiveness* of the whole, its taut yet springy resilience, that responds to small input in a way which is expressive of the whole. Shallow, wide, uncanny: that's the nature of biological intelligence.

#### .13

The "self" in that formula is only important because there is no locus of control: any given point is capable of avalanche.

The "organizing" is only important because it is the special property of critical order to produce that scale invariance which enables avalanche.

The term "self-organized" is misleading, and expressive of a certain histrionic awe before that which seems to defy our superstitions regarding agency, the mind, the soul - as though we were being generous with nature in this one instance.

An enormous number of inorganic processes can be said to "self-organize": maybe all of physics, maybe everything conceivable in the aftermath of the death of god. And here probably is the root of the matter: "organized" implies "somebody did it".

#### .14

Even Mandelbrot never seemed to ask himself what the distinct *design advantage* of fractalic structure might be: like most mathematicians, he's so impressed by the elegance of the encoding scheme that every other consideration fades from view. But that simplicity and rigor might actually be a hindrance in organic terms, since an organism may lose too much adaptive range for whatever savings in encoding is gained: it'd be worth asking the question why life didn't develop further along purely fractalic lines, but seemed to abandon it early on in favor of other symmetries.

It strikes me that the primary advantage of the fractal is not its encoding elegance but its *amplification potential*. Realized fractality acts inevitably as an amplifier of weak and partial signal. Like cracking a whip, the trick is to follow the gentle curve of its unfolding, to induce acceleration without exceeding its accumulating inertia. The cumulative signal grows logarithmically as it proceeds along the fractal scale: amplification response and generative formula coincide.

#### .15

This assumes that every scaling step, a proportional response grows the signal while matching the antennal sensitivity: such that we don't waste realized surface area for signal transduction. This reminds me of how surfers read waves: they sense which among the series has the best potential to match both the amplifying properties of the shoreline and the ambient perturbations of the medium - if there's too much amplitude early it'll crest before the contours of the seafloor can express themselves.

#### .16

My guess is that if we plotted the response in a naturally occuring fractal antenna, we'd find it was some proportion of Euler's number, a smooth growth function. The idea is that we don't want turbulence in growth. If we expend too much of the incoming signal - which is nothing but energy - too early in the response, we may fail to explore its full scope. We want a smooth distribution along our growth curve, gaining in amplitude and bandwidth while we eat into our signal budget. From the organic point of view, we don't know the value of any incoming information until it's been fully processed, but some signals are essential, so both efficiency and poised sensitivity are needed: fractal structure seems to allow for the gradual entrainment of some maximum of the network by simply scaling up, from the most responsive smaller scales to the larger less sensitive scales. This should also allow simultaneous signals to compete in parallel. Again during the signal event, neither the duration, nor the amplitude, nor the frequency band, nor the informative value is known until it's traversed.

This assumes that the signal is sufficiently uniform such that fractality has a chance to kick in - to utilize its own self-similar scaling to explore the signal without wasting time nor incoming energy. Imagine what happens when you flick a tuning fork: there's short but intense period of turbulent discharge, in which you've induced useless noise, before enough signal is dissipated that the fork can respond according to its resonant frequency: this is what we want to avoid.

#### .17

So the question becomes: is this a plausible model for the kind of neural excitation I have in the back of my mind? Namely myoclonus vs the "kundalini shivers":

* The kundalini shivers represent the remnant turbulence of an excursion past the critical point, triggered typically by inhalation.
* Myoclonic cascade represents the fallback, from some supercritical excitation to subcritical, triggered by exhalation.

It's worth emphasizing that these are merely *symptoms*, not the traversals themselves: these are merely turbulent signatures, vestigial signs. Keeping in mind that most of what happens in the nervous system is *invisible because it's efficient*: the only way to characterize it is via errata, excess noise, essentially *exhaust* - which is why overexcitation via psychedelia and deep meditation is so valuable. We need clues to how the smooth transduction event unfolds, the invisible induction and dissipative loss.

#### .18

There's a caveat that hasn't yet been expressed: criticality may not yield a grand unified theory of neural activity. It may only represent one possible mode: maybe calibration, maybe rare and expensive peak neural performance - it may not even be particularly adaptive to sustain. My own experience is that it's brief at best. On the other hand, it does seem that we're supposed to experience critical states every waking cycle, that we transition from sleep states of very low excitation to supercritical positions, that we should be more or less always on the move between these extremes, easily rising and gently falling - that would be the profile of a healthy creature. The larger context here is probably the wake-sleep cycle, and I rarely see this considered in all the talk about neural criticality: that we should be considering a continuum from the broadly coherent oscillations of deepest sleep to critical scale-invariant activation.

#### .19

<!-- TODO: where? -->

So we may call it "diagonal causality": neither fully vertical nor horizontal. This assumes that a certain orthogonality lies at the foundation of Democritean reductionism, which says that the sum of the interaction of similar parts determine the behavior of the whole: "horizontal" causality would be the foundation of Newtonian physics and every account of the result in terms of localized interactions between neighbors sharing the same scale, whereas "vertical" causality is the linear sum of these interactions across all scales. The diagonal causality of scale-invariant critical behavior defies these assumptions.

There's much more work to be done here. It turns out that outlining the psychological resistances involved in "diagonal causality" and nonlinear systems theory in general, requires a revision of the history of science - perhaps nothing less than a review of physics and mathematics since their origin in Babylon. And I do believe the issue is no less enormous: critical emergence challenges the very roots of postaxial Western thinking - or at least their *least Greek* roots.

#### .20

It's important for me to remember that I ended up in this path because of the need for a *viable mass psychology*. I didn't start with the cute appealing nature of fractals and criticality, and then go look for applications - Ã  la Hermann Haken and Per Bak. I come to this theory because similar vocabulary was growing out of [what I learned from COVID][friction] - namely what mass psychology is, how topdown causality is possible, what longrange correlation in the social sphere might be. I knew through experience that it was driven by frustration and alienation - which is nothing but turbulence in social terms: once I understood that such turbulence could create resonance and prepare the ground for an emergent order, I turned to criticality theory in earnest.

#### .21

I almost wish this theory weren't so attractive: "self-organized criticality", "chaos theory", the "entropic brain" and others like them continue to attract the wrong kind of attention - hasty popularization and a reactive dismissal in its wake. But the intuitions persist, and the Zeitgeist won't let it go: the 21st century wants an answer to the question, "what is intelligence?" And it expects an answer in terms of collective dynamics. But much more urgently - and yet I seem to be the only one aware of it as yet - this century begs the question: "what is mass psychology?" The answer to the one is the answer to the other: this is why I'm on this path at all.

---

This is a chapter from [Tapetum Lucidum][tapetum] which includes a previous piece [on fractals](/posts/why-fractals).

[friction]: /posts/maladaptive-friction/

[tapetum]: https://www.amazon.com/dp/1737889420
